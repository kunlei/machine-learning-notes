[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Notes",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "My journey of learning Machine Learning (ML).\n\nAn Introduction to Statistical Learning: with Applications in Python\nIntroduction to Machine Learning with Python: A Guide for Data Scientists",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "2  Environment Setup",
    "section": "",
    "text": "2.1 conda\nInstall conda from here:\nhttps://conda.io/projects/conda/en/latest/user-guide/install/macos.html",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#conda",
    "href": "setup.html#conda",
    "title": "2  Environment Setup",
    "section": "",
    "text": "# to list all available environments\nconda env list\n\n# to remove an environment\nconda env remove -n 'env_name'",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#machine-learning-packages",
    "href": "setup.html#machine-learning-packages",
    "title": "2  Environment Setup",
    "section": "2.2 Machine learning packages",
    "text": "2.2 Machine learning packages\nFirst install some of the essential libraries and tools\nconda install numpy scipy pandas matplotlib seaborn plotly ipython scikit-learn jupyterlab ipykernel pyarrow",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#add-conda-environment-into-notebook-kernel",
    "href": "setup.html#add-conda-environment-into-notebook-kernel",
    "title": "2  Environment Setup",
    "section": "2.3 Add conda environment into notebook kernel",
    "text": "2.3 Add conda environment into notebook kernel\n## Create the virtual environment\nconda create -n 'environment_name'\n\n## Activate the virtual environment\nconda activate 'environment_name'\n\n## Make sure that ipykernel is installed\npip install --user ipykernel\n\n## Add the new virtual environment to Jupyter\npython -m ipykernel install --user --name='environment_name'\n\n## To list existing Jupyter virtual environments\njupyter kernelspec list\n\n## To remove the environment from Jupyter\njupyter kernelspec uninstall 'environment_name'\n\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport scipy as sp\nimport IPython\nimport sklearn\nimport seaborn as sns\nimport plotly\n\nprint(f'Python version: {sys.version}')\nprint(f'numpy version: {np.__version__}')\nprint(f'pandas version: {pd.__version__}')\nprint(f'matplotlib version: {matplotlib.__version__}')\nprint(f'scipy version: {sp.__version__}')\nprint(f'IPython version: {IPython.__version__}')\nprint(f'sklearn version: {sklearn.__version__}')\nprint(f'seaborn version: {sns.__version__}')\nprint(f'plotly version: {plotly.__version__}')\n\nPython version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ]\nnumpy version: 1.26.2\npandas version: 2.1.4\nmatplotlib version: 3.7.2\nscipy version: 1.11.2\nIPython version: 8.15.0\nsklearn version: 1.3.2\nseaborn version: 0.13.1\nplotly version: 5.18.0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "explore-iris.html",
    "href": "explore-iris.html",
    "title": "3  Explore Iris Classification",
    "section": "",
    "text": "3.1 Classification\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((112, 4), (38, 4), (112,), (38,))\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=1)\ny_pred = knn.predict(X_test)\nknn.score(X_test, y_test)\n\n0.9736842105263158",
    "crumbs": [
      "Data and Feature Engineering",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explore Iris Classification</span>"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "4  Datasets",
    "section": "",
    "text": "4.1 The Wisconsin Breast Cancer dataset\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\ncancer = load_breast_cancer()\ndf_cancer = pd.DataFrame(data=cancer['data'], columns=cancer['feature_names'])\ndf_cancer['target'] = cancer['target']\ndf_cancer\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n0\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n0\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n0\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n0\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n1\n\n\n\n\n569 rows × 31 columns",
    "crumbs": [
      "Data and Feature Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "datasets.html#the-diabetes-dataset",
    "href": "datasets.html#the-diabetes-dataset",
    "title": "4  Datasets",
    "section": "4.2 The Diabetes Dataset",
    "text": "4.2 The Diabetes Dataset\n\nfrom sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\n\n\ndf_diabetes = pd.DataFrame(data=diabetes['data'], columns=diabetes['feature_names'])\ndf_diabetes['target'] = diabetes['target']\n\n\ndf_diabetes\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n437\n0.041708\n0.050680\n0.019662\n0.059744\n-0.005697\n-0.002566\n-0.028674\n-0.002592\n0.031193\n0.007207\n178.0\n\n\n438\n-0.005515\n0.050680\n-0.015906\n-0.067642\n0.049341\n0.079165\n-0.028674\n0.034309\n-0.018114\n0.044485\n104.0\n\n\n439\n0.041708\n0.050680\n-0.015906\n0.017293\n-0.037344\n-0.013840\n-0.024993\n-0.011080\n-0.046883\n0.015491\n132.0\n\n\n440\n-0.045472\n-0.044642\n0.039062\n0.001215\n0.016318\n0.015283\n-0.028674\n0.026560\n0.044529\n-0.025930\n220.0\n\n\n441\n-0.045472\n-0.044642\n-0.073030\n-0.081413\n0.083740\n0.027809\n0.173816\n-0.039493\n-0.004222\n0.003064\n57.0\n\n\n\n\n442 rows × 11 columns",
    "crumbs": [
      "Data and Feature Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "model-k-nearest-neighbors.html",
    "href": "model-k-nearest-neighbors.html",
    "title": "5  K-Nearest Neighbors",
    "section": "",
    "text": "5.1 KNN for Classification\nLoad data.\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\ncancer['data']\n\narray([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n        1.189e-01],\n       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n        8.902e-02],\n       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n        8.758e-02],\n       ...,\n       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n        7.820e-02],\n       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n        1.240e-01],\n       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n        7.039e-02]])\nSplit data.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(cancer['data'], cancer['target'], random_state=42)\nX_train.shape, y_train.shape\n\n((426, 30), (426,))\nX_test.shape, y_test.shape\n\n((143, 30), (143,))\nTrain model.\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier()\nknn.predict(X_test)\n\narray([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n       0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1])\nknn.predict_proba(X_test).shape\n\n(143, 2)\nScore model.\nknn.score(X_test, y_test)\n\n0.965034965034965\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\ncancer = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(cancer['data'], cancer['target'], random_state=42)\n\ntrain_scores, test_scores = [], []\n\nneighbor_candidates = range(1, 16)\nfor n_neighbors in neighbor_candidates:\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn.fit(X_train, y_train)\n    train_score = knn.score(X_train, y_train)\n    test_score = knn.score(X_test, y_test)\n\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n    \n\nfig, ax = plt.subplots(1, 1, figsize=(8, 3))\nax.plot(neighbor_candidates, train_scores, 'b-h', label='train accuracy')\nax.plot(neighbor_candidates, test_scores, 'r-*', label='test accuracy')\nax.set_xlabel('Number of neighbors')\nax.set_ylabel('Accuracy')\nax.set_ylim([0.9, 1.02])\nax.legend()\nfig.tight_layout()",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>K-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "model-k-nearest-neighbors.html#knn-for-regression",
    "href": "model-k-nearest-neighbors.html#knn-for-regression",
    "title": "5  K-Nearest Neighbors",
    "section": "5.2 KNN for Regression",
    "text": "5.2 KNN for Regression\n\nfrom sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes['data'], diabetes['target'], random_state=42)\n\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((331, 10), (111, 10), (331,), (111,))\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor(n_neighbors=3)\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\n\nknn.score(X_test, y_test)\n\n0.37222167132521977\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndiabetes = load_diabetes()\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes['data'], diabetes['target'], random_state=42)\n\ntrain_scores, test_scores = [], []\nneighbor_candidates = range(1, 51)\nfor n_neighbors in neighbor_candidates:\n    knn = KNeighborsRegressor(n_neighbors=n_neighbors)\n    knn.fit(X_train, y_train)\n\n    train_score = knn.score(X_train, y_train)\n    test_score = knn.score(X_test, y_test)\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 3))\nax.plot(neighbor_candidates, train_scores, 'b-h', label='train R squared')\nax.plot(neighbor_candidates, test_scores, 'r-*', label='test R squared')\nax.set_xlabel('Number of neighbors')\nax.set_ylabel('R squared')\nax.legend()\nfig.tight_layout()",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>K-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "model-linear-models.html",
    "href": "model-linear-models.html",
    "title": "6  Linear Models",
    "section": "",
    "text": "6.1 Linear Models for Regression\nIn linear regression models, we assume the observed data follows the function form of\n\\[\n\\hat{f}(x_i) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} + \\epsilon_i\n\\]\nwhere \\(\\beta_0 \\cdots \\beta_p\\) are the parameters to be estimated from observed data and \\(\\epsilon\\) is the irreducible error term associated with observation \\(x_i\\). Note that we use \\(p\\) to denote the number of features in the dataset.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "model-linear-models.html#linear-models-for-regression",
    "href": "model-linear-models.html#linear-models-for-regression",
    "title": "6  Linear Models",
    "section": "",
    "text": "6.1.1 Linear regression\nThe Ordinary Least Squares (OLS) is the most commonly used objective function to estimate the \\(\\beta\\) values. It is defined as\n\\[\n\\text{min.} \\sum_{i = 1}^{n} (y_i - \\hat{f}(x_i))^2\n\\]\nwhere \\(n\\) is the number of training samples.\n\n\n6.1.2 Ridge Regression\nIn ridge regression, a penalty term is added to the OLS to reduce the variation of \\(\\beta\\). Specifically, the objective function is defined as\n\\[\n\\text{min.} \\sum_{i = 1}^{n} (y_i - \\hat{f}(x_i))^2 + \\alpha \\sum_{j = 1}^p \\beta_j^2\n\\]\nwhere \\(\\alpha\\) is the hyperparameter that controls the extent to which we want to reduce the variance.\n\n\n6.1.3 Lasso Regression\nSimilar to Ridge regression, Lasso regression aims to reduce the number of features. Mathematically, the objective function is defined as\n\\[\n\\text{min.} \\sum_{i = 1}^{n} (y_i - \\hat{f}(x_i))^2 + \\alpha \\sum_{j = 1}^p |\\beta_j|\n\\]\nwhere \\(\\alpha\\) is the hyperparameter that controls the extent to which we want to reduce the variance.\n\n\n6.1.4 ElasticNet\nElastic-net considers both \\(l_1\\) and \\(l_2\\)-norm regularization in the objective function.\n\n\n6.1.5 Datasets\n\n6.1.5.1 A dummy dataset\n\nfrom sklearn.datasets import make_regression\n\ndummy_X, dummy_y = make_regression(n_samples=100, n_features=1, random_state=42, noise=20)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 3))\nax.plot(dummy_X[:, 0], dummy_y, 'bh')\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ndummy_X_train, dummy_X_test, dummy_y_train, dummy_y_test = train_test_split(dummy_X, dummy_y, random_state=42)\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(dummy_X_train, dummy_y_train)\n\nprint(f'coefficients: {lr.coef_}')\nprint(f'intercept: {lr.intercept_:.2f}')\n\ncoefficients: [46.97069029]\nintercept: -0.80\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 3))\nax.plot(dummy_X[:, 0], dummy_y, 'bh')\nax.plot(dummy_X_train, lr.predict(dummy_X_train), 'r--', linewidth=3)\n\n\n\n\n\n\n\n\n\nprint(f'train score: {lr.score(dummy_X_train, dummy_y_train):.2f}')\nprint(f'test score: {lr.score(dummy_X_test, dummy_y_test):.2f}')\n\ntrain score: 0.86\ntest score: 0.77\n\n\n\n\n6.1.5.2 The Diabetes dataset\n\nfrom sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\n\n\nimport pandas as pd\n\ndf_diabetes = pd.DataFrame(data=diabetes['data'], columns=diabetes['feature_names'])\ndf_diabetes['target'] = diabetes['target']\ndf_diabetes.head()\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n\n\n\n\n\n\n\ndiabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = train_test_split(diabetes['data'], diabetes['target'], random_state=42)\n\n\n\n\n6.1.6 Linear regression with scikit-learn\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(diabetes_X_train, diabetes_y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nprint(f'train score: {lr.score(diabetes_X_train, diabetes_y_train):.2f}')\nprint(f'test score: {lr.score(diabetes_X_test, diabetes_y_test):.2f}')\n\ntrain score: 0.52\ntest score: 0.48\n\n\n\n\n6.1.7 Ridge regression with scikit-learn\n\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge()\nridge.fit(diabetes_X_train, diabetes_y_train)\nprint(f'train score: {ridge.score(diabetes_X_train, diabetes_y_train):.2f}')\nprint(f'test score: {ridge.score(diabetes_X_test, diabetes_y_test):.2f}')\n\ntrain score: 0.43\ntest score: 0.44\n\n\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\nalpha_list = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = [], []\nfor alpha in alpha_list:\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(diabetes_X_train, diabetes_y_train)\n    train_scores.append(ridge.score(diabetes_X_train, diabetes_y_train))\n    test_scores.append(ridge.score(diabetes_X_test, diabetes_y_test))\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 3))\nax.plot(alpha_list, train_scores, label='train score')\nax.plot(alpha_list, test_scores, label='test score')\nax.set_xlabel('alpha')\nax.set_ylabel('R squared')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.1.8 Lasso regression with scikit-learn\n\nfrom sklearn.linear_model import Lasso\n\nlasso = Lasso()\nlasso.fit(diabetes_X_train, diabetes_y_train)\nprint(f'train score: {lasso.score(diabetes_X_train, diabetes_y_train):.2f}')\nprint(f'test score: {lasso.score(diabetes_X_test, diabetes_y_test):.2f}')\n\ntrain score: 0.35\ntest score: 0.38\n\n\n\nfrom sklearn.linear_model import Lasso\nimport matplotlib.pyplot as plt\n\nalpha_list = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = [], []\nfor alpha in alpha_list:\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(diabetes_X_train, diabetes_y_train)\n    train_scores.append(lasso.score(diabetes_X_train, diabetes_y_train))\n    test_scores.append(lasso.score(diabetes_X_test, diabetes_y_test))\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 3))\nax.plot(alpha_list, train_scores, label='train score')\nax.plot(alpha_list, test_scores, label='test score')\nax.set_xlabel('alpha')\nax.set_ylabel('R squared')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.1.9 ElasticNet with scikit-learn\n\nfrom sklearn.linear_model import ElasticNet\n\nelastic = ElasticNet()\nelastic.fit(diabetes_X_train, diabetes_y_train)\nprint(f'train score: {elastic.score(diabetes_X_train, diabetes_y_train):.2f}')\nprint(f'test score: {elastic.score(diabetes_X_test, diabetes_y_test):.2f}')\n\ntrain score: 0.01\ntest score: -0.00",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "model-linear-models.html#linear-models-for-classification",
    "href": "model-linear-models.html#linear-models-for-classification",
    "title": "6  Linear Models",
    "section": "6.2 Linear Models for Classification",
    "text": "6.2 Linear Models for Classification\n\n6.2.1 Datasets\n\n\n6.2.2 Dummy classification dataset\n\nfrom sklearn.datasets import make_classification\n\ndummy_clf_X, dummy_clf_y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=42)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.scatter(dummy_clf_X[:, 0], dummy_clf_X[:, 1], c=dummy_clf_y)\nax.set_xlabel('Feature 1')\nax.set_ylabel('Feature 2')\nfig.tight_layout()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\n\n\n6.2.3 The breast cancer dataset\n\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\n\n\n\n6.2.4 Logistic regression with scikit-learn\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.linear_model import LogisticRegression\n\nfeature_1, feature_2 = np.meshgrid(\n    np.linspace(dummy_clf_X[:, 0].min(), dummy_clf_X[:, 0].max()),\n    np.linspace(dummy_clf_X[:, 1].min(), dummy_clf_X[:, 1].max())\n)\n\ngrid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n\nclf = LogisticRegression()\nclf.fit(dummy_clf_X, dummy_clf_y)\n\ny_pred = np.reshape(clf.predict(grid), feature_1.shape)\ndisplay = DecisionBoundaryDisplay(xx0=feature_1, xx1=feature_2, response=y_pred)\ndisplay.plot()\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nbreast_X, breast_y = load_breast_cancer(return_X_y=True)\nbreast_train_X, breast_test_X, breast_train_y, breast_test_y = train_test_split(breast_X, breast_y, random_state=42)\n\nclf = LogisticRegression(C=1.0, max_iter=10000)\nclf.fit(breast_train_X, breast_train_y)\nprint(f'train score: {clf.score(breast_train_X, breast_train_y):.2f}')\nprint(f'test score: {clf.score(breast_test_X, breast_test_y):.2f}')\n\ntrain score: 0.96\ntest score: 0.97\n\n\nExplore the impact of the parameter C\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nbreast_X, breast_y = load_breast_cancer(return_X_y=True)\nbreast_train_X, breast_test_X, breast_train_y, breast_test_y = train_test_split(breast_X, breast_y, random_state=42)\n\nC_candidates = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\ntrain_scores, test_scores = [], []\nfor C in C_candidates:\n    clf = LogisticRegression(C=C, max_iter=100000)\n    clf.fit(breast_train_X, breast_train_y)\n    train_scores.append(clf.score(breast_train_X, breast_train_y))\n    test_scores.append(clf.score(breast_test_X, breast_test_y))\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nax.plot(C_candidates, train_scores, 'b--', label='train score')\nax.plot(C_candidates, test_scores, 'r--', label='test score')\nax.set_xlabel('C')\nax.set_ylabel('Accuracy')\nax.legend()\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n6.2.5 Linear support vector machine with scikit-learn\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import LinearSVC\n\nfeature_1, feature_2 = np.meshgrid(\n    np.linspace(dummy_clf_X[:, 0].min(), dummy_clf_X[:, 0].max()),\n    np.linspace(dummy_clf_X[:, 1].min(), dummy_clf_X[:, 1].max())\n)\n\ngrid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n\nclf = LinearSVC()\nclf.fit(dummy_clf_X, dummy_clf_y)\n\ny_pred = np.reshape(clf.predict(grid), feature_1.shape)\ndisplay = DecisionBoundaryDisplay(xx0=feature_1, xx1=feature_2, response=y_pred)\ndisplay.plot()\n\n/opt/homebrew/anaconda3/envs/ml-notes/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n6.2.6 Multiclass classification\n\nfrom sklearn.datasets import make_classification\n\ndummy_multi_clf_X, dummy_multi_clf_y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, n_classes=3, n_clusters_per_class=1, class_sep=3,  random_state=42)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.scatter(dummy_multi_clf_X[:, 0], dummy_multi_clf_X[:, 1], c=dummy_multi_clf_y)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfrom sklearn.svm import LinearSVC\n\nclf = LinearSVC(dual='auto')\nclf.fit(dummy_multi_clf_X, dummy_multi_clf_y)\n\nLinearSVC(dual='auto')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearSVC?Documentation for LinearSVCiFittedLinearSVC(dual='auto') \n\n\n\nclf.coef_, clf.intercept_\n\n(array([[-0.74236851,  0.8409381 ],\n        [ 0.7269626 ,  0.00635368],\n        [ 0.00214378, -0.79588124]]),\n array([-2.09831008,  0.08652886, -0.1261963 ]))\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.scatter(dummy_multi_clf_X[:, 0], dummy_multi_clf_X[:, 1], c=dummy_multi_clf_y)\nline = np.linspace(-5, 6)\nfor coef, intercept, color in zip(clf.coef_, clf.intercept_,\n                                      ['b', 'r', 'g']):\n        ax.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nax.set_xlim(-5, 6)\nax.set_ylim(-5, 6)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(dummy_multi_clf_X, dummy_multi_clf_y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.scatter(dummy_multi_clf_X[:, 0], dummy_multi_clf_X[:, 1], c=dummy_multi_clf_y)\nline = np.linspace(-5, 6)\nfor coef, intercept, color in zip(clf.coef_, clf.intercept_,\n                                      ['b', 'r', 'g']):\n        ax.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nax.set_xlim(-5, 6)\nax.set_ylim(-5, 6)\nfig.tight_layout()",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "model-naive-bayes.html",
    "href": "model-naive-bayes.html",
    "title": "7  Naive Bayes",
    "section": "",
    "text": "7.1 Gaussian Naive Bayes\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(f'train score: {clf.score(X_train, y_train):.2f}')\nprint(f'test score: {clf.score(X_test, y_test):.2f}')\n\ntrain score: 0.94\ntest score: 0.96",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "model-decision-trees.html",
    "href": "model-decision-trees.html",
    "title": "8  Decision Trees and Tree Ensembles",
    "section": "",
    "text": "8.1 Decision Tree",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision Trees and Tree Ensembles</span>"
    ]
  },
  {
    "objectID": "model-decision-trees.html#decision-tree",
    "href": "model-decision-trees.html#decision-tree",
    "title": "8  Decision Trees and Tree Ensembles",
    "section": "",
    "text": "8.1.1 Classification\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer['data'], cancer['target'], random_state=42)\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\nprint(f'train score: {clf.score(X_train, y_train):.2f}')\nprint(f'test score: {clf.score(X_test, y_test):.2f}')\n\ntrain score: 1.00\ntest score: 0.94\n\n\n\nclf = DecisionTreeClassifier(max_depth=3)\nclf.fit(X_train, y_train)\nprint(f'train score: {clf.score(X_train, y_train):.2f}')\nprint(f'test score: {clf.score(X_test, y_test):.2f}')\n\ntrain score: 0.97\ntest score: 0.96\n\n\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nexport_graphviz(clf, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\nfeature_names=cancer.feature_names, impurity=False, filled=True)\nwith open(\"tree.dot\") as f: \n    dot_graph = f.read()\n\ngraphviz.Source(dot_graph)\n\n\n\n\n\n\n\n\n\nclf.feature_importances_\n\narray([0.        , 0.02845995, 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.76145947, 0.        , 0.        ,\n       0.0139744 , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.06454691, 0.06233829, 0.03145043, 0.03777055, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ])\n\n\n\n\n8.1.2 Feature Importance\n\nimport shap\n\nexplainer = shap.Explainer(clf.predict, X_train)\nshap_values = explainer(X_test)\nshap.plots.waterfall(shap_values[0])\n\n\n\n\n\n\n\n\n\nshap.plots.beeswarm(shap_values)\n\n\n\n\n\n\n\n\n\n\n8.1.3 Regression\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\ndiabetes = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(diabetes['data'], diabetes['target'], random_state=42)\ntree = DecisionTreeRegressor(max_depth=5)\ntree.fit(X_train, y_train)\nprint(f'train score: {tree.score(X_train, y_train):.2f}')\nprint(f'test score: {tree.score(X_test, y_test):.2f}')\n\ntrain score: 0.69\ntest score: 0.23\n\n\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nexport_graphviz(tree, out_file=\"tree.dot\",\nfeature_names=diabetes.feature_names, impurity=False, filled=True)\nwith open(\"tree.dot\") as f: \n    dot_graph = f.read()\n\ngraphviz.Source(dot_graph)\n\n\n\n\n\n\n\n\n\nimport shap\n\nexplainer = shap.Explainer(tree.predict, X_train)\nshap_values = explainer(X_test)\nshap.plots.waterfall(shap_values[0])",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision Trees and Tree Ensembles</span>"
    ]
  },
  {
    "objectID": "model-decision-trees.html#random-forest",
    "href": "model-decision-trees.html#random-forest",
    "title": "8  Decision Trees and Tree Ensembles",
    "section": "8.2 Random Forest",
    "text": "8.2 Random Forest\n\n8.2.1 Classification\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nclf = RandomForestClassifier(n_estimators=100, max_features=6, n_jobs=-1)\nclf.fit(X_train, y_train)\nprint(f'train score: {clf.score(X_train, y_train):.2f}')\nprint(f'test score: {clf.score(X_test, y_test):.2f}')\n\ntrain score: 1.00\ntest score: 0.97\n\n\n\nclf.estimators_[0]\n\nDecisionTreeClassifier(max_features=6, random_state=1620431822)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_features=6, random_state=1620431822) \n\n\n\nimport shap\n\nexplainer = shap.Explainer(clf.predict, X_train)\nshap_values = explainer(X_test)\nshap.plots.waterfall(shap_values[0])\n\nPermutationExplainer explainer: 144it [00:19,  3.60it/s]                         \n\n\n\n\n\n\n\n\n\n\n\n8.2.2 Regression\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nX, y = load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nreg = RandomForestRegressor(n_estimators=100, max_features=6, n_jobs=-1)\nreg.fit(X_train, y_train)\nprint(f'train score: {reg.score(X_train, y_train):.2f}')\nprint(f'test score: {reg.score(X_test, y_test):.2f}')\n\ntrain score: 0.92\ntest score: 0.47\n\n\n\nimport shap\n\nexplainer = shap.Explainer(reg.predict, X_train)\nshap_values = explainer(X_test)\nshap.plots.waterfall(shap_values[0])",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision Trees and Tree Ensembles</span>"
    ]
  },
  {
    "objectID": "model-decision-trees.html#gradient-boosting-machines",
    "href": "model-decision-trees.html#gradient-boosting-machines",
    "title": "8  Decision Trees and Tree Ensembles",
    "section": "8.3 Gradient Boosting Machines",
    "text": "8.3 Gradient Boosting Machines\n\n8.3.1 Classification\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\nclf.fit(X_train, y_train)\nprint(f'train score: {clf.score(X_train, y_train):.2f}')\nprint(f'test score: {clf.score(X_test, y_test):.2f}')\n\ntrain score: 1.00\ntest score: 0.96\n\n\n\n\n8.3.2 Regression\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nX, y = load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nreg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\nreg.fit(X_train, y_train)\nprint(f'train score: {reg.score(X_train, y_train):.2f}')\nprint(f'test score: {reg.score(X_test, y_test):.2f}')\n\ntrain score: 0.85\ntest score: 0.42",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision Trees and Tree Ensembles</span>"
    ]
  }
]